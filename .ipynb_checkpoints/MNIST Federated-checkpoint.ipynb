{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0214a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from threading import Thread\n",
    "from multiprocessing import Process, Manager\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a1fb7",
   "metadata": {},
   "source": [
    "# Create the model that will be instantiated for the workers and the master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69517386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        # input is 28x28\n",
    "        # padding=2 for same padding\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        # feature map size is 14*14 by pooling\n",
    "        # padding=2 for same padding\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        # feature map size is 7*7 by pooling\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*7*7)   # reshape Variable\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d25fb",
   "metadata": {},
   "source": [
    "# Define the master model, number of workers and instantiate them\n",
    "Also we define the batch size and optimizers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a67d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_model = MnistModel()\n",
    "optimizer = optim.SGD(master_model.parameters(), lr=0.01, momentum=0.9) # Defining optimizer for master model\n",
    "\n",
    "worker_size = 3\n",
    "workers = []\n",
    "optimizers = []\n",
    "for i in range(worker_size):\n",
    "    workers.append(MnistModel())\n",
    "    optimizers.append(optim.SGD(workers[i].parameters(), lr=0.01, momentum=0.9)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7616a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the waiting to random.random() or 0 depending on Task 1 or Task 2\n",
    "waiting_time = 0.0\n",
    "waiting_time = random.random() #random between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc12325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd198d4",
   "metadata": {},
   "source": [
    "# Get the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3245b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "# !tar -zxvf MNIST.tar.gz\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cba35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df77cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71277e03",
   "metadata": {},
   "source": [
    "# Worker Loop\n",
    "For each worker, perform a forward pass over the batch and record the loss.\n",
    "After getting the forward pass loss, calculate the gradients over the backward pass and store them.\n",
    "Do the same steps for all the workers and sum the gradients of each layer of the model into one dictionary called params.\n",
    "## The code is parallel since we're using a multiprocessing system to imitate several workers\n",
    "This is achieved by using the multiprocessing library offered by python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5972be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "params = manager.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3564379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_forward(worker,index,params):\n",
    "    time.sleep(waiting_time+index) # Use in Task 2 (change param in beginning of notebook)\n",
    "    for data, target in train_loader: # at every iteration it generates a new batch\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizers[index].zero_grad()\n",
    "        output = worker(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()    # calc gradients\n",
    "        for param_index,param in enumerate(worker.parameters()):\n",
    "            if param_index not in params:\n",
    "                params[param_index] = param.grad\n",
    "            else:\n",
    "                params[param_index] += param.grad \n",
    "        break # We do it over 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94ecce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_workers():\n",
    "    for index, worker in enumerate(workers):\n",
    "        if(index == 0):\n",
    "            worker_forward (worker,index,params) #This is done because we need to initialize some params of the shared dictionary. #This adds a latency of only 1 device which is okay for the sake of demonstration.\n",
    "        else:\n",
    "            process = Process(target = worker_forward, args = (worker,index,params, ))\n",
    "            process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe67024",
   "metadata": {},
   "source": [
    "# Master Model Loop\n",
    "Once the params of the nodes have been aggregated, we update the gradients of the worker models since we have added the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff9ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't use this function since at the end we're performing inference on the worker\n",
    "def update_master_gradients():\n",
    "    for param_index,param in enumerate(master_model.parameters()):\n",
    "        param.grad = params[param_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90c4bf",
   "metadata": {},
   "source": [
    "Once we update the master model gradients, we perform one step of the optimizer to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f861338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_master_weights():\n",
    "    master_model.train()\n",
    "    optimizer.step()   # update gradients\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591ef50",
   "metadata": {},
   "source": [
    "# Most Importantly update the worker weights by the aggregated gradient of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b5f2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_worker_weights():\n",
    "    for index, optim in enumerate(optimizers):\n",
    "        workers[index].train()\n",
    "        optim.step() #do a step for each optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d61cdd",
   "metadata": {},
   "source": [
    "# To do it properly we aggregate all the previous parts together and we do it over multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80882ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "params = manager.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fe3fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_temp_params(): # Only do it after master gradient has been updated\n",
    "    params = manager.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb631ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 200/200 [02:20<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 140.5531461238861 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in tqdm(range(200)):\n",
    "    forward_pass_workers()\n",
    "    update_master_gradients()\n",
    "    update_worker_weights()\n",
    "    clear_temp_params()\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dc6e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(worker):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for data, target in test_loader:\n",
    "        count += batch_size\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = worker(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "        \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, count,\n",
    "        100. * correct / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "890e9284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2250, Accuracy: 9282/10016 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(workers[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
